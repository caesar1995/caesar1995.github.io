---
title: "Load Test with HTTPS causes memory leak and OOM?"
date: 2019-02-25
tags: [.NET Framework, OOM, HTTPS, HttpWebRequest]
excerpt: "A memory leak issue in HttpWebRequest when communicating with a HTTPS server through a proxy."
---

One of our customers wants to run a complex integrated performance test suite testing a collection of systems supporting external customer access for their clients (a major NZ government department). The memory leak prevented them from running the full suite of tests which they normally run for significant releases of their customer's systems. In particular, it was not possible to complete long duration tests, because of the Out of Memory issue on the test rig.

## What is the Issue?

When a HttpWebRequest wants to initiate a connection to a HTTPS server through a proxy, it will create a TunnelStateObject, which contains connection information. In our code, after finishing network communication, we forget to free the TunnelStateObject. This will result in related Connection objects staying in memory, eventually will lead to OOM issue.

## Deep Analysis

To summarize the issue: client sends a HttpWebRequest to server through a proxy. Proxy modifies server’s response with “Connection: close”, to make client free up the connection resources (like socket, request, response, stream, string, byte[], connection…). If we make this single scenario as a load test, (like a stress test: create multiple clients sending HWRs with unlimited bandwidth for a certain amount of time), we will find out that the GC of the connection resources is way slower than creating new connections, which will eventually lead to system runs out of memory. The picture below demonstrates our current design:

Click to expand:

[ ![overview](\assets\post_pics\oom.jpg) ](\assets\post_pics\oom.jpg)

If we are using the proxy when sending HttpWebRequest to a https server, our code will create two HWR, and two Connections.

The problem in our code is that, we eventually use Connection2 to get the Response from the proxy (which sits on our local machine), while Connection2 doesn’t detect the KeepAliveFailure (here is the problem). Therefore, Conneciton2 is not idle and thinks itself will be reused, which prevents itself from being GCed. But the truth is, Connection1 did detect the KeepAliveFailure, and close the underneath socket after getting the Response. And this socket is shared by both Connection1 and Connection2. So the situation is: Connection2 thinks itself will be reused in the future, and remains open; however, it’s impossible to get reused, because the underneath socket is disconnected and cleaned up by Connection1. 

And, because of the dependency chain: Socket -> Connection1 -> TunnelObject -> Connection2. If the deepest/lowest component, which is Connection2, still remains active, it will prevent the dead Connection1 and socket being get Garbage Collected. Until the Connection2 is get timeout.

Therefore, to summary the OOM issue: our code doesn’t respect the “Connection: close” header if: 1. Remote server uses https 2. Proxy is used. In our customer’s very specific scenario, unfortunately, these two conditions are met. Because the nature of stress test (creating too many Connections), the memory will eventually run out.

## The Fix

To fix the issue, we need to find a way to notify Connection2 about KeepAliveFailure, and remove it from ConnectionGroup. So I made the HttpWebRequest contains a reference to the TunnelConnection. Before disposing the current HttpWebRequest, remove the TunnelConnection from ConnectionList, so that it can be garbage collected.

## Verification Steps

It’s hard to add test cases (which requires computer to set up with: stress test & proxy & “Connection:close” header) for the fix. Therefore, I attached the screenshots for the same repro Before & After fix, taken at 30 seconds and 1 minute, as a verification for the fix.

Here is the test code I wrote:
```c#
class Program
{
    static void Main(string[] args)
    {
        //reproCase();

        Task task1 = Task.Factory.StartNew(() => reproCase());
        Task task2 = Task.Factory.StartNew(() => reproCase());
        Task task3 = Task.Factory.StartNew(() => reproCase());

        Task task10 = Task.Factory.StartNew(() => reproCase());
        Task task20 = Task.Factory.StartNew(() => reproCase());
        Task task30 = Task.Factory.StartNew(() => reproCase());

        Task task11 = Task.Factory.StartNew(() => reproCase());
        Task task21 = Task.Factory.StartNew(() => reproCase());
        Task task31 = Task.Factory.StartNew(() => reproCase());

        Task task12 = Task.Factory.StartNew(() => reproCase());
        Task task22 = Task.Factory.StartNew(() => reproCase());
        Task task32 = Task.Factory.StartNew(() => reproCase());

        Task.WaitAll(task1, task2, task3, task10, task20, task30, task11, task21, task31, task12, task22, task32);
        Console.WriteLine("All threads complete");
    }

    static void reproCase()
    {
        while (true)
        {
            HttpWebRequest request = (HttpWebRequest)WebRequest.Create("https://www.????"); // Any Https sites.
            request.Method = "GET";
            request.Headers.Add(HttpRequestHeader.AcceptLanguage, "en-NZ");
            request.Headers.Add(HttpRequestHeader.AcceptEncoding, "gzip, deflate");

            var task = request.GetResponseAsync();
            HttpWebResponse response = (HttpWebResponse)task.Result;

            StreamReader reader = new StreamReader(response.GetResponseStream());
            string body = reader.ReadToEnd();

            //response.Dispose(); //Either read/dispose the Response.
        }
    }
}
```

### Before the fix:

At 30 seconds: 

My repro process takes 357MB of memory.

[ ![overview](\assets\post_pics\before30.jpg) ](\assets\post_pics\before30.jpg)

At 1 minute:

The memory consumption has grown from 357 MB -> 547 MB within 30 seconds!

[ ![overview](\assets\post_pics\before60.jpg) ](\assets\post_pics\before60.jpg)

### After the fix:

At 30 seconds:

The process only consumes 65 MB memory.

[ ![overview](\assets\post_pics\after30.jpg) ](\assets\post_pics\after30.jpg)

At 1 minute:

The repro process is very stable, with a little growth from 65MB to 66MB.

[ ![overview](\assets\post_pics\after60.jpg) ](\assets\post_pics\after60.jpg)

Now we are confident that the OOM issue is gone!
